# Ollama LLM ğŸ¤–

OlÃ¡, saudaÃ§Ãµes!

Este projeto tem como finalidade explorar boas prÃ¡ticas de programaÃ§Ã£o, alÃ©m de aplicar conhecimentos especÃ­ficos na linguagem Python. Utilizamos FastAPI para criar um servidor web e TinyLlama para realizar requests a um modelo de linguagem local. Este trabalho estÃ¡ sendo desenvolvido no contexto de um projeto pessoal para facilitar interaÃ§Ãµes com inteligÃªncia artificial em tempo real.

### ğŸš€ ComeÃ§ando

Essas instruÃ§Ãµes permitirÃ£o que vocÃª obtenha uma cÃ³pia do projeto em operaÃ§Ã£o na sua mÃ¡quina local para fins de desenvolvimento e teste.

Consulte ImplantaÃ§Ã£o para saber como implantar o projeto.

### ğŸ“‹ PrÃ©-requisitos & ğŸ”§ InstalaÃ§Ã£o

Para instalar e executar este projeto, vocÃª precisarÃ¡ do Python instalado em sua mÃ¡quina.

    Verifique se vocÃª possui o Python instalado:
        VocÃª pode verificar a versÃ£o do Python com o seguinte comando:

    bash

```
python --version
```

Instale as dependÃªncias do projeto:

    No terminal, execute o seguinte comando:

bash
```
pip install -r requirements.txt
```

Execute o servidor:

    ApÃ³s a instalaÃ§Ã£o, vocÃª pode iniciar o servidor FastAPI com o seguinte comando:

bash
```
    uvicorn main:app --reload
```

```
    Acesse a API:
        Abra seu navegador e acesse http://localhost:8000/docs para visualizar a documentaÃ§Ã£o interativa da API.
```

### ğŸ”© AnÃ¡lise de Testes de Funcionalidade

```
Terminal
```

Teste de Resposta do Modelo:

```
    CenÃ¡rio:
        Um usuÃ¡rio faz uma requisiÃ§Ã£o para o modelo de linguagem.
        O servidor processa a solicitaÃ§Ã£o e retorna uma resposta gerada pelo modelo.
```

Teste de Performance:

```
    CenÃ¡rio:
        MÃºltiplas requisiÃ§Ãµes simultÃ¢neas sÃ£o feitas ao servidor.
    VerificaÃ§Ã£o:
        Confirma se o servidor consegue lidar com vÃ¡rias requisiÃ§Ãµes ao mesmo tempo.
        Garante que as respostas sÃ£o geradas em um tempo aceitÃ¡vel.
```

### ğŸ› ï¸ ConstruÃ­do com

Mencione as ferramentas que vocÃª usou para criar seu projeto

    Python - Linguagem de ProgramaÃ§Ã£o
    FastAPI - Framework para ConstruÃ§Ã£o de APIs
    TinyLlama - Modelo de Linguagem para Respostas Locais

### ğŸ–‡ï¸ Colaborando

Por favor, leia o COLABORACAO.md para obter detalhes sobre o nosso cÃ³digo de conduta e o processo para nos enviar pedidos de solicitaÃ§Ã£o.

### ğŸ“Œ VersÃ£o

(0.1.0) - 26-10-2024 (ElaboraÃ§Ã£o Inicial do Servidor) <br> (0.2.0) - 28-10-2024 (ImplementaÃ§Ã£o de Respostas da IA)

### âœ’ï¸ Autores

Mencione todos aqueles que ajudaram a levantar o projeto desde o seu inÃ­cio

    Pedro Henrique (EU) - Desenvolvedor do Ollama LLM

VocÃª tambÃ©m pode ver a lista de todos os colaboradores que participaram deste projeto.

### ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a (sua licenÃ§a) - veja o arquivo LICENSE.md para detalhes.

âŒ¨ï¸ com â¤ï¸ por PedrÃ£o Ribeiro ğŸ˜Š
